# What is Generative AI
- **Versatile inputs:** Generative AI can process a wide range of data, including text, images, sounds, animations, and 3D models.
- **Creative outputs:** It can generate new content in various formats, such as text, images, sounds, and more.
## How Does Generative AI Work
- Uses neural networks to identify patterns in existing data.
- Generates new, original content based on these patterns.

	### Breakthrough: Unsupervised/Semi-supervised Learning

	- Leverages unlabeled data for training.
	- Creates foundation models more efficiently.

	### Foundation Models: Building Blocks for AI

	- Serve as a base for various AI tasks.
	- Examples: GPT-3, Stable Diffusion.

	### Applications of Foundation Models

	- GPT-3: Text generation (e.g., ChatGPT).
	- Stable Diffusion: Image generation.
## How to Evaluate Generative AI Models?
	
	### Quality
	
	- **High-quality outputs:** Essential for user interaction.
	- **Example:** Speech generation, image generation.
	
	### Diversity
	
	- **Captures minority modes:** Reduces biases.
	- **Preserves quality:** Maintains generation quality.
	
	### Speed
	
	- **Fast generation:** Necessary for interactive applications.
	- **Example:** Real-time image editing.
## How to Develop Generative AI Models?

	## Diffusion Models: A Breakdown
	
	### How Diffusion Models Work
	
	**Training Process:** Diffusion models use a two-step process:
	**Forward diffusion:** Gradually adds random noise to the training data.
	**Reverse diffusion:** Reverses the noise to reconstruct the data samples.
	**Novel Data Generation:** New data is generated by starting with random noise and running the reverse denoising process.
	
	### Advantages of Diffusion Models
	
	**High-quality output:** Offer the best quality output among generative AI models.
	**Flexibility:** Can be used for various tasks.
	**Foundation models:** Large-scale, high-quality, flexible, and suitable for general use cases.
	
	### Disadvantages of Diffusion Models
	
	**Training time:** Take longer to train compared to VAEs.
	**Slow inference:** Generating new data is a slow and lengthy process due to reverse sampling.
## Variational Autoencoders (VAEs)

	### How VAEs Work:

	- **Encoder:** Converts input into a smaller, compressed representation.
	- **Decoder:** Reconstructs original input from compressed representation.
	- **Goal:** Learn efficient latent data representation.

	### Benefits of VAEs:

	- Faster generation of outputs (like images).

	### Limitations of VAEs:

	- Generated images may not be as detailed as those from diffusion models.
## Generative Adversarial Networks (GANs)

	### How GANs Work
	
	- **Training:** Two models (generator and discriminator) are trained simultaneously.
	- **Iteration:** The generator produces content, and the discriminator evaluates its authenticity.
	- **Improvement:** Both models learn from each other, improving with each iteration.
	- **Goal:** The generator aims to create content indistinguishable from real data.
	
	### Limitations of GANs
	
	- **Sample diversity:** GANs struggle to produce diverse samples.
	- **Domain-specific:** They are best suited for generating data within a specific domain.
	
	### Transformer Networks
	
	- **Architecture:** A popular architecture for generative models.
	- **Understanding:** It's crucial to understand how transformers work in the context of generative AI.
